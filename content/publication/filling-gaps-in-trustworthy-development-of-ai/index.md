---
# Documentation: https://sourcethemes.com/academic/docs/managing-content/

title: "Filling gaps in trustworthy development of AI"
authors:
- Shahar Avin
- Haydn Belfield
- Miles Brundage
- Gretchen Krueger
- Jasmine Wang
- Adrian Weller
- Markus Anderljung
- Igor Krawczuk
- David Krueger
- Jonathan Lebensbold
- Tegan Maharaj
- Noa Zilberman
date: 2021-12-09T00:00:00+00:00
doi: "https://doi.org/10.1126/science.abi7176"

# Schedule page publish date (NOT publication's date).
publishDate: 2021-12-09T00:00:00+00:00

# Publication type.
# Legend: 0 = Uncategorized; 1 = Conference paper; 2 = Journal article;
# 3 = Preprint / Working Paper; 4 = Report; 5 = Book; 6 = Book section;
# 7 = Thesis; 8 = Patent
publication_types: ["2"]

# Publication name and optional abbreviated publication name.
publication: "In *Science*"
publication_short: ""

abstract: "The range of application of artificial intelligence (AI) is vast, as is the potential for harm. Growing awareness of potential risks from AI systems has spurred action to address those risks while eroding confidence in AI systems and the organizations that develop them. A 2019 study (1) found more than 80 organizations that have published and adopted “AI ethics principles,” and more have joined since. But the principles often leave a gap between the “what” and the “how” of trustworthy AI development. Such gaps have enabled questionable or ethically dubious behavior, which casts doubts on the trustworthiness of specific organizations, and the field more broadly. There is thus an urgent need for concrete methods that both enable AI developers to prevent harm and allow them to demonstrate their trustworthiness through verifiable behavior. Below, we explore mechanisms [drawn from (2)] for creating an ecosystem where AI developers can earn trust—if they are trustworthy (see the figure). Better assessment of developer trustworthiness could inform user choice, employee actions, investment decisions, legal recourse, and emerging governance regimes."

# Summary. An optional shortened abstract.
summary: "Incident sharing, auditing, and other concrete mechanisms could help verify the trustworthiness of actors"

tags: ["AI"]
categories: []
featured: true

# Custom links (optional).
#   Uncomment and edit lines below to show custom links.
# links:
# - name: Follow
#   url: https://twitter.com
#   icon_pack: fab
#   icon: twitter

url_pdf: https://www.science.org/doi/reader/10.1126/science.abi7176
url_preprint: https://arxiv.org/abs/2112.07773
url_code:
url_dataset:
url_poster:
url_project:
url_slides:
url_source:
url_video:

links:
- name: Gizmodo
  url: https://gizmodo.com/secretive-ai-companies-need-to-cooperate-with-good-hack-1848185849

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder. 
# Focal points: Smart, Center, TopLeft, Top, TopRight, Left, Right, BottomLeft, Bottom, BottomRight.
image:
  caption: ""
  focal_point: ""
  preview_only: false

# Associated Projects (optional).
#   Associate this publication with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `internal-project` references `content/project/internal-project/index.md`.
#   Otherwise, set `projects: []`.
projects: []

# Slides (optional).
#   Associate this publication with Markdown slides.
#   Simply enter your slide deck's filename without extension.
#   E.g. `slides: "example"` references `content/slides/example/index.md`.
#   Otherwise, set `slides: ""`.
slides: ""
---
